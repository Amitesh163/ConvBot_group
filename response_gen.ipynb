{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "response_gen",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0GpboadhvjL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "051aa4b4-e11b-47f1-9b8a-4f3808d53d80"
      },
      "source": [
        "! wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-05 10:27:16--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3663376519 (3.4G) [application/gzip]\n",
            "Saving to: ‘en.txt.gz’\n",
            "\n",
            "en.txt.gz           100%[===================>]   3.41G  8.97MB/s    in 6m 23s  \n",
            "\n",
            "2020-07-05 10:33:42 (9.11 MB/s) - ‘en.txt.gz’ saved [3663376519/3663376519]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad2KSe1sh2V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip -k en.txt.gz\n",
        "!mkdir lines\n",
        "!split -a 3 -l 100000 en.txt lines/lines-"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_9bsu1Uur-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a4e890a8-be57-48c3-bca7-bf0b05b9953b"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "zip"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XIhHwKUh7NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def _should_skip(line, min_length, max_length):\n",
        "    \"\"\"Whether a line should be skipped depending on the length.\"\"\"\n",
        "    return len(line) < min_length or len(line) > max_length\n",
        "\n",
        "\n",
        "def create_example(previous_lines, line, file_id):\n",
        "    \"\"\"Creates examples with multi-line context\n",
        "    The examples will include:\n",
        "        file_id: the name of the file where these lines were obtained.\n",
        "        response: the current line text\n",
        "        context: the previous line text\n",
        "        context/0: 2 lines before\n",
        "        context/1: 3 lines before, etc.\n",
        "    \"\"\"\n",
        "    example = {\n",
        "        'file_id': file_id,\n",
        "        'context': previous_lines[-1],\n",
        "        'response': line,\n",
        "    }\n",
        "    example['file_id'] = file_id\n",
        "    example['context'] = previous_lines[-1]\n",
        "\n",
        "    extra_contexts = previous_lines[:-1]\n",
        "    example.update({\n",
        "        'context/{}'.format(i): context\n",
        "        for i, context in enumerate(extra_contexts[::-1])\n",
        "    })\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "def _preprocess_line(line):\n",
        "    line = line.decode(\"utf-8\")\n",
        "\n",
        "    # Remove the first word if it is followed by colon (speaker names)\n",
        "    # NOTE: this wont work if the speaker's name has more than one word\n",
        "    line = re.sub('(?:^|(?:[.!?]\\\\s))(\\\\w+):', \"\", line)\n",
        "\n",
        "    # Remove anything between brackets (corresponds to acoustic events).\n",
        "    line = re.sub(\"[\\\\[(](.*?)[\\\\])]\", \"\", line)\n",
        "\n",
        "    # Strip blanks hyphens and line breaks\n",
        "    line = line.strip(\" -\\n\")\n",
        "\n",
        "    return line\n",
        "\n",
        "\n",
        "def _create_examples_from_file(file_name, min_length=0, max_length=20,\n",
        "                               num_extra_contexts=5):\n",
        "\n",
        "    previous_lines = []\n",
        "    with open(file_name, 'rb') as f:\n",
        "      for line in f :\n",
        "        line = _preprocess_line(line)\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        should_skip = _should_skip(\n",
        "            line,\n",
        "            min_length=min_length,\n",
        "            max_length=max_length)\n",
        "\n",
        "        if previous_lines:\n",
        "            should_skip |= _should_skip(\n",
        "                previous_lines[-1],\n",
        "                min_length=min_length,\n",
        "                max_length=max_length)\n",
        "\n",
        "            if not should_skip:\n",
        "                yield create_example(previous_lines, line, file_name)\n",
        "\n",
        "        previous_lines.append(line.lower())\n",
        "        if len(previous_lines) > num_extra_contexts + 1:\n",
        "            del previous_lines[0]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w4YNXZAh_tA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0ee5deba-2782-48c8-d744-a0bcf4282f30"
      },
      "source": [
        "example = _create_examples_from_file(file_name='lines/lines-aaa')\n",
        "\n",
        "count = 0\n",
        "for i in example:\n",
        "  count += 1\n",
        "print('Found '+ str(count*5) + ' examples')\n",
        "print(i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 101770 examples\n",
            "{'file_id': 'lines/lines-aaa', 'context': \"we'll catch him.\", 'response': 'Because we have to.', 'context/0': 'oh, aye.', 'context/1': \"tell me, sergeant, in your professional opinion, do you really believe you can catch this man if he doesn't want you to?\", 'context/2': 'i have.', 'context/3': 'you seem to have made a great study of crime, mr. newspaperman.', 'context/4': \"the ones we don't know about yet.\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-kkQyUqiD0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "24192eda-998a-4a2e-88b8-a3e59cee1c5d"
      },
      "source": [
        "in_comma = \"'\"\n",
        "\n",
        "def remove_char(sentence):\n",
        "  sent = sentence.replace('!', '')\n",
        "  sent = sent.replace(',', '')\n",
        "  sent = sent.replace(in_comma, '')\n",
        "  sent = sent.replace('%', '')\n",
        "  sent = sent.replace('-', '')\n",
        "  sent = sent.replace('.', '')\n",
        "  sent = sent.replace('?', '')\n",
        "  sent = sent.replace('/', '')\n",
        "  sent = sent.replace(':', '')\n",
        "  sent = sent.replace(';', '')\n",
        "\n",
        "  return sent\n",
        "\n",
        "ex_sent = 'Hello, me! why?'\n",
        "print(remove_char(ex_sent).lower())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello me why\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpGhXejdiF-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3e24aece-4bd9-4eee-bb19-74b317c2d13e"
      },
      "source": [
        "\n",
        "example = _create_examples_from_file(file_name='lines/lines-aaa')\n",
        "\n",
        "inputs = []\n",
        "responses = []\n",
        "i = 0\n",
        "\n",
        "for test in example:\n",
        "\n",
        "  input_1 = test['context']\n",
        "  input_2 = test['context/0']\n",
        "  input_3 = test['context/1']\n",
        "  input_4 = test['context/2']\n",
        "  input_5 = test['context/3']\n",
        "  response = test['response']\n",
        "    \n",
        "  inputs.append(remove_char(input_1))\n",
        "  inputs.append(remove_char(input_2))\n",
        "  inputs.append(remove_char(input_3))\n",
        "  inputs.append(remove_char(input_4))\n",
        "  inputs.append(remove_char(input_5))\n",
        "\n",
        "  for j in range(5):\n",
        "    responses.append('startsent' + ' ' +remove_char(response.lower())+ ' ' + 'endsent')\n",
        "    \n",
        "  i += 5\n",
        "\n",
        "print(inputs[101769], responses[101769])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you seem to have made a great study of crime mr newspaperman startsent because we have to endsent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxbdFoWOiJEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "oov_token = \"<OOV>\"\n",
        "max_length = 20\n",
        "num_topic_words = 4\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(inputs)\n",
        "tokenizer.fit_on_texts(responses)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "word_index['startsent'] = 0\n",
        "word_index['endsent'] = len(word_index)+1\n",
        "index_word = {word_index[word]:word for word in word_index}\n",
        "#print(index_word)\n",
        "vocab_size = len(word_index) + 1\n",
        "input_seq = tokenizer.texts_to_sequences(inputs)\n",
        "response_seq = tokenizer.texts_to_sequences(responses)\n",
        "input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "response_seq_pad = pad_sequences(response_seq, maxlen = max_length*2, padding = 'post', truncating = 'post')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH6cZxoGiqn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sent(text_list):\n",
        "  inputs = []\n",
        "  for sent in text_list:\n",
        "    inputs.append(remove_char(sent))\n",
        "  input_seq = tokenizer.texts_to_sequences(inputs)\n",
        "  input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "  return input_seq_pad  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkjELBB5ffuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "aa40331e-4745-403d-86a5-55a25cc6d965"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-05 10:45:20--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
            "--2020-07-05 10:45:20--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip [following]\n",
            "--2020-07-05 10:45:21--  http://downloads.cs.stanford.edu/nlp/data/glove.42B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877800501 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  2.09MB/s    in 14m 35s \n",
            "\n",
            "2020-07-05 10:59:56 (2.05 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tVGz_cwoP5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ac247bd2-d65b-40bb-e65b-9dbd240f2c4d"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "with ZipFile('glove.42B.300d.zip', 'r') as mad: \n",
        "  mad.extractall() \n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zipfile.ZipFile [closed]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PgXCCdOtElA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c06f31a2-f68d-4b96-e518-d6d13abe1ae3"
      },
      "source": [
        ""
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zipfile.ZipFile [closed]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egdlS4h2f37A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e4da88e8-a84c-4c72-989b-b10fa7a2c77f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {} \n",
        "f = open('/content/glove.42B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1917494 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGKwG8tiX22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cd51a3f-1df4-46de-806a-8139c0b43ea0"
      },
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))            \n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector\n",
        "\n",
        "print(len(embeddings_matrix))\n",
        "embeddings_matrix[6977]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.36530000e-01, -3.35099995e-01,  6.26939982e-02, -2.94600010e-01,\n",
              "       -2.73090005e-01,  9.11000013e-01, -2.82599998e+00,  5.84870018e-02,\n",
              "       -4.92949992e-01,  1.42220005e-01,  3.02810013e-01,  7.64840007e-01,\n",
              "       -3.16020012e-01, -5.04109979e-01, -2.34589994e-01,  2.64039993e-01,\n",
              "       -1.02569997e-01,  3.87710005e-01, -2.36750007e-01,  1.19010001e-01,\n",
              "        9.07519996e-01,  4.25880015e-01, -1.00560002e-01,  7.10859969e-02,\n",
              "       -6.39040023e-02,  8.33090007e-01, -1.13660000e-01, -5.71449995e-01,\n",
              "       -2.29409993e-01,  4.60310012e-01, -4.93079990e-01, -7.80960023e-02,\n",
              "        7.69479990e-01, -5.64499974e-01,  1.93869993e-01,  4.83650006e-02,\n",
              "       -1.18450001e-01, -1.75060004e-01,  3.09949994e-01,  7.11000025e-01,\n",
              "        1.14929996e-01, -4.95620012e-01, -2.78039992e-01, -4.06749994e-01,\n",
              "       -4.02310014e-01,  4.94029999e-01,  1.76760003e-01,  3.64100009e-01,\n",
              "       -2.72269994e-01,  1.20540000e-01, -2.18170002e-01,  2.84779996e-01,\n",
              "       -4.83669996e-01,  1.87059999e-01,  9.65619981e-02, -3.82880002e-01,\n",
              "       -4.63250011e-01, -6.61889985e-02, -2.83569992e-01,  6.06909990e-01,\n",
              "       -5.15739977e-01,  4.17430013e-01, -8.87459964e-02,  7.54920021e-02,\n",
              "       -2.52829995e-02,  7.19780028e-01,  2.09969997e-01, -3.51689994e-01,\n",
              "       -1.85829997e-01,  1.32760003e-01,  1.96559995e-01, -2.11440003e-03,\n",
              "        3.03420007e-01, -1.88309997e-01,  4.90810007e-01,  3.38759989e-01,\n",
              "       -4.30180013e-01, -1.06930003e-01,  2.92589992e-01,  7.74379969e-02,\n",
              "       -2.37460002e-01, -2.32219994e-01,  2.84469992e-01,  3.13970000e-01,\n",
              "        5.83599985e-01, -6.87050000e-02,  4.88940001e-01,  2.77029991e-01,\n",
              "       -4.47959989e-01, -2.47630000e-01, -1.53310001e-01, -1.00440003e-01,\n",
              "        2.85879999e-01, -1.77090004e-01, -9.10070017e-02, -1.03489995e+00,\n",
              "       -1.98860002e+00, -1.87869996e-01,  6.90229982e-03,  2.45260000e-01,\n",
              "       -4.71789986e-01, -3.17629993e-01,  2.09590003e-01,  3.22409987e-01,\n",
              "       -5.64520001e-01,  2.32879996e-01,  1.92699999e-01,  2.72410005e-01,\n",
              "       -6.04789972e-01,  3.88460010e-01, -3.68279994e-01, -2.28929996e-01,\n",
              "        2.65560001e-01, -1.63340002e-01,  2.03669995e-01, -7.57759988e-01,\n",
              "       -3.66730005e-01, -5.46310008e-01, -7.72610009e-02, -3.81639987e-01,\n",
              "        7.77890027e-01, -1.40860006e-01,  9.12889987e-02, -8.76119971e-01,\n",
              "       -1.34580001e-01, -1.86590001e-01,  4.25829999e-02, -2.37859994e-01,\n",
              "        5.29799998e-01, -6.64739981e-02,  5.84399998e-01, -1.12269998e-01,\n",
              "        4.77990001e-01, -5.23119986e-01, -2.88709998e-01,  2.25309998e-01,\n",
              "       -3.43210012e-01, -4.09249991e-01,  1.72069997e-01, -3.46989989e-01,\n",
              "        6.96439981e-01, -8.58349979e-01, -9.71099973e-01,  3.85610014e-01,\n",
              "       -1.48800001e-01,  3.25749993e-01,  2.64219999e-01, -4.44959998e-01,\n",
              "       -8.07269990e-01, -1.14859998e-01,  1.75280005e-01, -4.04720008e-01,\n",
              "       -6.14069998e-01,  5.31750023e-01,  5.84480017e-02, -7.12119997e-01,\n",
              "       -4.45179999e-01, -5.83519995e-01, -2.29690000e-01,  7.22109973e-02,\n",
              "        3.21020007e-01, -1.74369998e-02,  2.67699987e-01,  2.67349988e-01,\n",
              "       -6.46340027e-02,  2.77579995e-03,  2.79949993e-01, -1.76310003e-01,\n",
              "       -5.57690024e-01,  5.47479987e-01, -1.99449994e-02,  1.84149995e-01,\n",
              "       -4.15809989e-01, -3.77699994e-02, -4.02429998e-02,  3.12790006e-01,\n",
              "        6.99699998e-01, -5.96599989e-02, -1.50179997e-01,  1.40389994e-01,\n",
              "        6.71140015e-01,  3.61690015e-01,  3.66100013e-01, -1.14299998e-01,\n",
              "       -8.65930021e-01, -3.22409987e-01, -2.70649999e-01,  8.04010034e-02,\n",
              "       -8.61579999e-02, -4.95950013e-01, -1.24920001e-02, -3.09929997e-01,\n",
              "       -2.17549995e-01, -1.11199999e+00,  1.17699997e-02,  2.76609987e-01,\n",
              "        1.44910002e-02,  2.01079994e-01, -1.87260002e-01,  8.54180008e-02,\n",
              "        3.83080006e-01,  2.56949991e-01, -1.11330003e-01,  4.92810011e-02,\n",
              "       -2.50809997e-01,  5.10460019e-01, -7.06129968e-02, -2.63289988e-01,\n",
              "       -5.80089986e-01, -2.13890001e-01,  3.49159986e-01,  4.41529989e-01,\n",
              "        6.99069977e-01, -1.74070001e-01,  1.01089999e-01,  9.17599976e-01,\n",
              "       -5.53499982e-02,  1.50629997e-01,  2.47030005e-01, -3.09300005e-01,\n",
              "        2.37479992e-02,  4.83609997e-02, -3.67659986e-01,  1.65230006e-01,\n",
              "       -1.91250002e+00,  6.12169981e-01, -1.71969995e-01, -1.14459999e-01,\n",
              "       -6.08810008e-01,  3.53639983e-02,  2.27579996e-01,  1.32939994e-01,\n",
              "       -5.44870019e-01,  1.70230001e-01,  3.19050014e-01,  2.56900012e-01,\n",
              "       -2.78159995e-02,  7.28280023e-02,  3.62140000e-01, -3.22750002e-01,\n",
              "       -5.78819990e-01,  1.81529999e-01,  1.27450004e-01, -6.49290025e-01,\n",
              "       -3.33139986e-01,  4.71109986e-01,  2.61020008e-02, -1.62349999e-01,\n",
              "        2.73339987e-01, -7.95769989e-02, -2.61290014e-01,  2.31879994e-01,\n",
              "        6.64600015e-01, -7.54069984e-01, -3.80199999e-01, -2.29409993e-01,\n",
              "        5.57709992e-01, -6.32929981e-01, -1.62689999e-01, -7.12810010e-02,\n",
              "        7.48480037e-02,  3.07350010e-01,  1.43059995e-03,  1.00170001e-01,\n",
              "       -7.30690022e-04,  7.14949965e-02,  1.50279999e-01, -7.05640018e-01,\n",
              "       -4.49440002e-01,  9.74129979e-03,  3.48580003e-01,  3.16109993e-02,\n",
              "        1.82329997e-01,  3.87840003e-01, -6.94499984e-02,  1.13980003e-01,\n",
              "       -5.65800011e-01, -4.56739992e-01,  7.89900005e-01,  1.62520006e-01,\n",
              "       -7.05810010e-01, -3.71650010e-01, -1.10280000e-01,  4.82219994e-01,\n",
              "       -6.73179984e-01,  6.58890009e-01,  2.82079995e-01, -1.06810004e-01,\n",
              "       -1.09999999e-01,  1.83129996e-01,  3.37040015e-02,  4.48229998e-01,\n",
              "        8.81360006e-03, -3.39419991e-01,  1.59060001e-01,  8.72209966e-02,\n",
              "       -1.80059999e-01,  2.45890006e-01, -4.44970012e-01,  2.34430000e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBzcw_i-jKTV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e3e16a75-a7c6-4c96-f2db-d321aa0b438e"
      },
      "source": [
        "#Implementing Lda topic modeling\n",
        "#Splitting the inputs list itens word wise\n",
        "topic_train = []\n",
        "for i in range(len(inputs)):\n",
        "  topic_train.append(inputs[i].split())\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "#Filter out stop words in English \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "for i in range(len(topic_train)):\n",
        "  sent=\" \".join(topic_train[i])\n",
        "  sent=nlp(sent)\n",
        "  topic_train[i] = [ stemmer.stem(w.lemma_) for w in sent if w.text.lower() not in stops and len(w)>2]\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymZcOTGw0epk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dc17aaf1-318b-4f17-a3ff-e22b0893208c"
      },
      "source": [
        "#Forming dictionary out of the words\n",
        "from gensim import corpora\n",
        "topic_dict = corpora.Dictionary(topic_train)\n",
        "topic_dict.filter_extremes(no_below=10, no_above=0.5)\n",
        "\n",
        "#Obtaining bow_corpus out of the topics\n",
        "bow_corpus = [topic_dict.doc2bow(doc) for doc in topic_train]\n",
        "\n",
        "#Getting tfidf corpus from tfidf model\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "#training the lda model\n",
        "from gensim import models\n",
        "NUM_TOPICS = 30\n",
        "NUM_PASSES = 60 \n",
        "ALPHA = 'auto'\n",
        "ETA = 'auto'\n",
        "# Train LDA model\n",
        "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, \n",
        "                                         num_topics=NUM_TOPICS, \n",
        "                                         id2word=topic_dict,\n",
        "                                         passes=NUM_PASSES, \n",
        "                                         alpha=ALPHA, \n",
        "                                         eta=ETA,\n",
        "                                         random_state=200)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTO_3gt5rGil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0277e88e-b338-427e-daa1-3af819cad323"
      },
      "source": [
        "print(inputs[5],responses[4])\n",
        "print(inputs[5:10],responses[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here we go startsent here we go endsent\n",
            "['here we go', 'im going to wait', 'we promised to meet here two years later but she hasnt come yet', 'exactly two years ago today she and i buried a time capsule here', 'my sassy girl'] startsent here we go endsent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9oVqGSr0I83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a77b4b74-5100-4828-f7c2-93fda0eb25b8"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "\n",
        "print(topic_train[0])\n",
        "\n",
        "#get the scores of the topics generated\n",
        "for index, score in sorted(lda_model[bow_corpus[4]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(19,\n",
            "  '0.119*\"hand\" + 0.099*\"minut\" + 0.051*\"jump\" + 0.048*\"fast\" + 0.044*\"murder\" '\n",
            "  '+ 0.040*\"peopl\" + 0.035*\"kiss\" + 0.033*\"save\" + 0.030*\"air\" + '\n",
            "  '0.029*\"realiz\"'),\n",
            " (14,\n",
            "  '0.139*\"understand\" + 0.125*\"wait\" + 0.113*\"die\" + 0.091*\"woman\" + '\n",
            "  '0.072*\"saito\" + 0.054*\"first\" + 0.040*\"war\" + 0.032*\"punish\" + '\n",
            "  '0.023*\"prepar\" + 0.021*\"pretti\"'),\n",
            " (22,\n",
            "  '0.200*\"bridg\" + 0.136*\"pleas\" + 0.100*\"move\" + 0.042*\"life\" + 0.042*\"build\" '\n",
            "  '+ 0.042*\"possibl\" + 0.038*\"spend\" + 0.036*\"veronica\" + 0.032*\"someon\" + '\n",
            "  '0.030*\"husband\"'),\n",
            " (6,\n",
            "  '0.244*\"tell\" + 0.085*\"tri\" + 0.081*\"finish\" + 0.079*\"readi\" + 0.075*\"never\" '\n",
            "  '+ 0.038*\"studi\" + 0.035*\"chapman\" + 0.031*\"captain\" + 0.031*\"kid\" + '\n",
            "  '0.027*\"fault\"'),\n",
            " (13,\n",
            "  '0.154*\"back\" + 0.111*\"talk\" + 0.065*\"bori\" + 0.063*\"idea\" + 0.054*\"end\" + '\n",
            "  '0.054*\"beauti\" + 0.049*\"job\" + 0.047*\"goodby\" + 0.036*\"ever\" + '\n",
            "  '0.035*\"interest\"'),\n",
            " (24,\n",
            "  '0.303*\"see\" + 0.181*\"help\" + 0.157*\"sorri\" + 0.049*\"babi\" + 0.029*\"intrud\" '\n",
            "  '+ 0.028*\"fall\" + 0.025*\"1000\" + 0.019*\"honest\" + 0.019*\"case\" + '\n",
            "  '0.018*\"lucki\"'),\n",
            " (3,\n",
            "  '0.164*\"thing\" + 0.120*\"yeah\" + 0.077*\"care\" + 0.075*\"hear\" + 0.069*\"turn\" + '\n",
            "  '0.031*\"plan\" + 0.026*\"yet\" + 0.025*\"principl\" + 0.024*\"lover\" + '\n",
            "  '0.022*\"fellow\"'),\n",
            " (21,\n",
            "  '0.145*\"mad\" + 0.098*\"may\" + 0.056*\"question\" + 0.044*\"ask\" + 0.037*\"better\" '\n",
            "  '+ 0.033*\"lot\" + 0.027*\"post\" + 0.026*\"terribl\" + 0.025*\"pill\" + '\n",
            "  '0.024*\"occur\"'),\n",
            " (25,\n",
            "  '0.125*\"anyth\" + 0.074*\"indian\" + 0.063*\"sergeant\" + 0.051*\"mrs\" + '\n",
            "  '0.049*\"hurri\" + 0.038*\"jen\" + 0.035*\"dead\" + 0.029*\"alon\" + 0.029*\"serious\" '\n",
            "  '+ 0.027*\"miss\"'),\n",
            " (18,\n",
            "  '0.380*\"thank\" + 0.145*\"work\" + 0.084*\"use\" + 0.076*\"sit\" + 0.040*\"reev\" + '\n",
            "  '0.031*\"hous\" + 0.021*\"brass\" + 0.020*\"ahoy\" + 0.019*\"death\" + '\n",
            "  '0.016*\"phone\"'),\n",
            " (17,\n",
            "  '0.178*\"time\" + 0.080*\"three\" + 0.072*\"okay\" + 0.071*\"stay\" + 0.045*\"mani\" + '\n",
            "  '0.040*\"still\" + 0.039*\"ahead\" + 0.035*\"day\" + 0.027*\"shoot\" + 0.026*\"arm\"'),\n",
            " (5,\n",
            "  '0.120*\"offic\" + 0.105*\"take\" + 0.054*\"order\" + 0.050*\"alway\" + '\n",
            "  '0.049*\"wrong\" + 0.046*\"darl\" + 0.037*\"gold\" + 0.031*\"quit\" + 0.028*\"hard\" + '\n",
            "  '0.027*\"bad\"'),\n",
            " (15,\n",
            "  '0.506*\"yes\" + 0.056*\"cours\" + 0.056*\"mind\" + 0.048*\"mine\" + 0.041*\"stand\" + '\n",
            "  '0.028*\"money\" + 0.025*\"congratul\" + 0.024*\"dinner\" + 0.023*\"long\" + '\n",
            "  '0.012*\"ingeni\"'),\n",
            " (20,\n",
            "  '0.062*\"sick\" + 0.060*\"someth\" + 0.047*\"escap\" + 0.045*\"respons\" + '\n",
            "  '0.037*\"new\" + 0.036*\"fine\" + 0.031*\"list\" + 0.031*\"doctor\" + 0.031*\"send\" + '\n",
            "  '0.028*\"medic\"'),\n",
            " (8,\n",
            "  '0.282*\"come\" + 0.217*\"man\" + 0.077*\"sure\" + 0.059*\"shear\" + 0.046*\"littl\" + '\n",
            "  '0.030*\"polic\" + 0.024*\"armi\" + 0.023*\"attent\" + 0.019*\"quick\" + '\n",
            "  '0.017*\"pay\"'),\n",
            " (7,\n",
            "  '0.242*\"well\" + 0.200*\"let\" + 0.150*\"think\" + 0.067*\"do\" + 0.036*\"forget\" + '\n",
            "  '0.028*\"democrat\" + 0.026*\"frank\" + 0.022*\"mark\" + 0.018*\"foot\" + '\n",
            "  '0.014*\"line\"'),\n",
            " (26,\n",
            "  '0.198*\"kill\" + 0.109*\"ill\" + 0.102*\"look\" + 0.090*\"love\" + 0.082*\"joyc\" + '\n",
            "  '0.069*\"even\" + 0.048*\"easi\" + 0.034*\"aliv\" + 0.031*\"drink\" + 0.029*\"tea\"'),\n",
            " (29,\n",
            "  '0.197*\"know\" + 0.156*\"one\" + 0.095*\"show\" + 0.060*\"mean\" + 0.057*\"stop\" + '\n",
            "  '0.052*\"find\" + 0.031*\"place\" + 0.030*\"lie\" + 0.026*\"herbert\" + '\n",
            "  '0.025*\"thomson\"'),\n",
            " (10,\n",
            "  '0.269*\"good\" + 0.182*\"say\" + 0.053*\"name\" + 0.040*\"feel\" + 0.037*\"morn\" + '\n",
            "  '0.030*\"speak\" + 0.027*\"kind\" + 0.025*\"jolli\" + 0.024*\"lose\" + '\n",
            "  '0.024*\"figur\"'),\n",
            " (0,\n",
            "  '0.414*\"sir\" + 0.095*\"want\" + 0.051*\"hold\" + 0.045*\"realli\" + 0.034*\"along\" '\n",
            "  '+ 0.032*\"girl\" + 0.022*\"slave\" + 0.022*\"late\" + 0.022*\"swim\" + '\n",
            "  '0.016*\"guess\"')]\n",
            "['go', 'wait']\n",
            "Score: 0.046229783445596695\t Topic: 0.119*\"hand\" + 0.099*\"minut\" + 0.051*\"jump\" + 0.048*\"fast\" + 0.044*\"murder\"\n",
            "Score: 0.04588134214282036\t Topic: 0.414*\"sir\" + 0.095*\"want\" + 0.051*\"hold\" + 0.045*\"realli\" + 0.034*\"along\"\n",
            "Score: 0.0455239899456501\t Topic: 0.269*\"good\" + 0.182*\"say\" + 0.053*\"name\" + 0.040*\"feel\" + 0.037*\"morn\"\n",
            "Score: 0.04287216067314148\t Topic: 0.197*\"know\" + 0.156*\"one\" + 0.095*\"show\" + 0.060*\"mean\" + 0.057*\"stop\"\n",
            "Score: 0.0421462319791317\t Topic: 0.198*\"kill\" + 0.109*\"ill\" + 0.102*\"look\" + 0.090*\"love\" + 0.082*\"joyc\"\n",
            "Score: 0.0408271960914135\t Topic: 0.242*\"well\" + 0.200*\"let\" + 0.150*\"think\" + 0.067*\"do\" + 0.036*\"forget\"\n",
            "Score: 0.040651943534612656\t Topic: 0.282*\"come\" + 0.217*\"man\" + 0.077*\"sure\" + 0.059*\"shear\" + 0.046*\"littl\"\n",
            "Score: 0.03856351226568222\t Topic: 0.062*\"sick\" + 0.060*\"someth\" + 0.047*\"escap\" + 0.045*\"respons\" + 0.037*\"new\"\n",
            "Score: 0.03768279030919075\t Topic: 0.506*\"yes\" + 0.056*\"cours\" + 0.056*\"mind\" + 0.048*\"mine\" + 0.041*\"stand\"\n",
            "Score: 0.03761214390397072\t Topic: 0.120*\"offic\" + 0.105*\"take\" + 0.054*\"order\" + 0.050*\"alway\" + 0.049*\"wrong\"\n",
            "Score: 0.03535863384604454\t Topic: 0.178*\"time\" + 0.080*\"three\" + 0.072*\"okay\" + 0.071*\"stay\" + 0.045*\"mani\"\n",
            "Score: 0.03497537598013878\t Topic: 0.387*\"get\" + 0.123*\"would\" + 0.118*\"leav\" + 0.048*\"knife\" + 0.037*\"luck\"\n",
            "Score: 0.03248131647706032\t Topic: 0.090*\"colonel\" + 0.081*\"major\" + 0.075*\"warden\" + 0.071*\"two\" + 0.069*\"mayb\"\n",
            "Score: 0.03104054369032383\t Topic: 0.386*\"right\" + 0.126*\"way\" + 0.078*\"night\" + 0.070*\"happen\" + 0.044*\"hut\"\n",
            "Score: 0.03097979538142681\t Topic: 0.144*\"call\" + 0.095*\"must\" + 0.075*\"away\" + 0.051*\"code\" + 0.049*\"suppos\"\n",
            "Score: 0.030745510011911392\t Topic: 0.297*\"go\" + 0.089*\"tonight\" + 0.087*\"put\" + 0.049*\"happi\" + 0.049*\"tomorrow\"\n",
            "Score: 0.030644766986370087\t Topic: 0.144*\"hello\" + 0.132*\"could\" + 0.120*\"give\" + 0.056*\"keep\" + 0.054*\"clipton\"\n",
            "Score: 0.030165277421474457\t Topic: 0.076*\"blow\" + 0.067*\"lieuten\" + 0.057*\"old\" + 0.049*\"mama\" + 0.048*\"matter\"\n",
            "Score: 0.028751220554113388\t Topic: 0.163*\"like\" + 0.095*\"noth\" + 0.075*\"need\" + 0.068*\"train\" + 0.065*\"carri\"\n",
            "Score: 0.028277065604925156\t Topic: 0.126*\"blast\" + 0.115*\"hit\" + 0.096*\"stella\" + 0.077*\"gyeonwoo\" + 0.051*\"doubt\"\n",
            "Score: 0.028109455481171608\t Topic: 0.145*\"make\" + 0.106*\"much\" + 0.089*\"command\" + 0.055*\"boy\" + 0.049*\"live\"\n",
            "Score: 0.0278458334505558\t Topic: 0.380*\"thank\" + 0.145*\"work\" + 0.084*\"use\" + 0.076*\"sit\" + 0.040*\"reev\"\n",
            "Score: 0.027702689170837402\t Topic: 0.125*\"anyth\" + 0.074*\"indian\" + 0.063*\"sergeant\" + 0.051*\"mrs\" + 0.049*\"hurri\"\n",
            "Score: 0.02715652994811535\t Topic: 0.145*\"mad\" + 0.098*\"may\" + 0.056*\"question\" + 0.044*\"ask\" + 0.037*\"better\"\n",
            "Score: 0.02694256789982319\t Topic: 0.164*\"thing\" + 0.120*\"yeah\" + 0.077*\"care\" + 0.075*\"hear\" + 0.069*\"turn\"\n",
            "Score: 0.026598449796438217\t Topic: 0.303*\"see\" + 0.181*\"help\" + 0.157*\"sorri\" + 0.049*\"babi\" + 0.029*\"intrud\"\n",
            "Score: 0.02656427025794983\t Topic: 0.154*\"back\" + 0.111*\"talk\" + 0.065*\"bori\" + 0.063*\"idea\" + 0.054*\"end\"\n",
            "Score: 0.02627013437449932\t Topic: 0.244*\"tell\" + 0.085*\"tri\" + 0.081*\"finish\" + 0.079*\"readi\" + 0.075*\"never\"\n",
            "Score: 0.025773221626877785\t Topic: 0.200*\"bridg\" + 0.136*\"pleas\" + 0.100*\"move\" + 0.042*\"life\" + 0.042*\"build\"\n",
            "Score: 0.02562623843550682\t Topic: 0.139*\"understand\" + 0.125*\"wait\" + 0.113*\"die\" + 0.091*\"woman\" + 0.072*\"saito\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg_DfhYX-FNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "df8e5983-98c1-4bb3-9b00-60a310ad6741"
      },
      "source": [
        "'''#Implementing Lda topic modeling\n",
        "#Splitting the inputs list itens word wise\n",
        "topic_train = []\n",
        "for i in range(len(inputs)):\n",
        "  topic_train.append(inputs[i].split())\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "#Filter out stop words in English \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "for i in range(len(topic_train)):\n",
        "  sent=\" \".join(topic_train[i])\n",
        "  sent=nlp(sent)\n",
        "  topic_train[i] = [ w.lemma_ for w in sent if w.text.lower() not in stops and len(w)>2]\n",
        "\n",
        "'''"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oOiamDr-wcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "022ef5bd-2773-47a9-e978-6d6e3446b102"
      },
      "source": [
        "#Forming dictionary out of the words\n",
        "from gensim import corpora\n",
        "topic_dict = corpora.Dictionary(topic_train)\n",
        "topic_dict.filter_extremes(no_below=10, no_above=0.5)\n",
        "\n",
        "#Obtaining bow_corpus out of the topics\n",
        "bow_corpus = [topic_dict.doc2bow(doc) for doc in topic_train]\n",
        "\n",
        "#Getting tfidf corpus from tfidf model\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "#training the lda model\n",
        "from gensim import models\n",
        "NUM_TOPICS = 30\n",
        "NUM_PASSES = 60 \n",
        "ALPHA = 'auto'\n",
        "ETA = 'auto'\n",
        "# Train LDA model\n",
        "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, \n",
        "                                         num_topics=NUM_TOPICS, \n",
        "                                         id2word=topic_dict,\n",
        "                                         passes=NUM_PASSES, \n",
        "                                         alpha=ALPHA, \n",
        "                                         eta=ETA,\n",
        "                                         random_state=200)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL-wpK9W-0xz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69727e3d-e556-421d-d3ed-1ee90c307cf1"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "\n",
        "print(topic_train[0])\n",
        "\n",
        "#get the scores of the topics generated\n",
        "for index, score in sorted(lda_model[bow_corpus[4]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1,\n",
            "  '0.229*\"help\" + 0.150*\"understand\" + 0.101*\"hear\" + 0.099*\"woman\" + '\n",
            "  '0.080*\"darle\" + 0.042*\"people\" + 0.035*\"head\" + 0.030*\"realize\" + '\n",
            "  '0.020*\"piece\" + 0.017*\"maam\"'),\n",
            " (25,\n",
            "  '0.131*\"name\" + 0.105*\"warden\" + 0.076*\"wonder\" + 0.063*\"list\" + '\n",
            "  '0.042*\"report\" + 0.039*\"shut\" + 0.035*\"group\" + 0.034*\"principle\" + '\n",
            "  '0.033*\"fault\" + 0.026*\"wilderness\"'),\n",
            " (3,\n",
            "  '0.130*\"colonel\" + 0.075*\"mama\" + 0.073*\"hope\" + 0.071*\"lovely\" + '\n",
            "  '0.069*\"boy\" + 0.069*\"believe\" + 0.053*\"jolly\" + 0.045*\"stella\" + '\n",
            "  '0.044*\"shall\" + 0.027*\"tired\"'),\n",
            " (8,\n",
            "  '0.167*\"back\" + 0.120*\"talk\" + 0.084*\"feel\" + 0.070*\"forget\" + '\n",
            "  '0.064*\"suppose\" + 0.060*\"catch\" + 0.032*\"meg\" + 0.030*\"sort\" + '\n",
            "  '0.024*\"fever\" + 0.023*\"dysentery\"'),\n",
            " (11,\n",
            "  '0.067*\"soldier\" + 0.057*\"gold\" + 0.049*\"kind\" + 0.047*\"hit\" + '\n",
            "  '0.046*\"beautiful\" + 0.045*\"quite\" + 0.043*\"together\" + 0.043*\"first\" + '\n",
            "  '0.040*\"car\" + 0.039*\"horse\"'),\n",
            " (5,\n",
            "  '0.168*\"show\" + 0.083*\"maybe\" + 0.073*\"order\" + 0.059*\"many\" + 0.034*\"light\" '\n",
            "  '+ 0.032*\"nice\" + 0.031*\"meet\" + 0.028*\"jenkin\" + 0.023*\"able\" + '\n",
            "  '0.018*\"couple\"'),\n",
            " (20,\n",
            "  '0.358*\"let\" + 0.073*\"bring\" + 0.070*\"stand\" + 0.051*\"democratic\" + '\n",
            "  '0.039*\"study\" + 0.024*\"pick\" + 0.023*\"polytechnic\" + 0.021*\"interest\" + '\n",
            "  '0.019*\"here\" + 0.018*\"baker\"'),\n",
            " (6,\n",
            "  '0.158*\"could\" + 0.112*\"may\" + 0.105*\"do\" + 0.055*\"idea\" + 0.045*\"baby\" + '\n",
            "  '0.044*\"figure\" + 0.042*\"reasonable\" + 0.028*\"crack\" + 0.027*\"occur\" + '\n",
            "  '0.026*\"sleep\"'),\n",
            " (18,\n",
            "  '0.148*\"work\" + 0.085*\"use\" + 0.077*\"course\" + 0.068*\"evening\" + '\n",
            "  '0.050*\"saito\" + 0.048*\"train\" + 0.031*\"battalion\" + 0.030*\"serious\" + '\n",
            "  '0.029*\"english\" + 0.026*\"pill\"'),\n",
            " (0,\n",
            "  '0.130*\"please\" + 0.113*\"sure\" + 0.093*\"hand\" + 0.085*\"really\" + '\n",
            "  '0.060*\"british\" + 0.060*\"girl\" + 0.034*\"veronica\" + 0.032*\"post\" + '\n",
            "  '0.030*\"commando\" + 0.029*\"husband\"'),\n",
            " (13,\n",
            "  '0.386*\"get\" + 0.309*\"thank\" + 0.053*\"ready\" + 0.048*\"knife\" + '\n",
            "  '0.048*\"matter\" + 0.035*\"doctor\" + 0.033*\"reeve\" + 0.010*\"wake\" + '\n",
            "  '0.008*\"position\" + 0.007*\"joe\"'),\n",
            " (14,\n",
            "  '0.196*\"one\" + 0.066*\"find\" + 0.062*\"night\" + 0.062*\"hold\" + '\n",
            "  '0.046*\"responsible\" + 0.039*\"place\" + 0.036*\"ahead\" + 0.035*\"hut\" + '\n",
            "  '0.024*\"another\" + 0.022*\"dig\"'),\n",
            " (26,\n",
            "  '0.128*\"ill\" + 0.118*\"call\" + 0.078*\"must\" + 0.065*\"sit\" + 0.059*\"love\" + '\n",
            "  '0.055*\"sergeant\" + 0.052*\"never\" + 0.048*\"along\" + 0.042*\"fine\" + '\n",
            "  '0.040*\"live\"'),\n",
            " (15,\n",
            "  '0.254*\"say\" + 0.231*\"go\" + 0.069*\"something\" + 0.059*\"madness\" + '\n",
            "  '0.056*\"finish\" + 0.048*\"radio\" + 0.046*\"hey\" + 0.042*\"hurry\" + '\n",
            "  '0.034*\"moment\" + 0.030*\"listen\"'),\n",
            " (21,\n",
            "  '0.303*\"right\" + 0.255*\"well\" + 0.081*\"yeah\" + 0.073*\"mean\" + 0.059*\"put\" + '\n",
            "  '0.034*\"hell\" + 0.019*\"rule\" + 0.017*\"follow\" + 0.014*\"line\" + '\n",
            "  '0.013*\"honour\"'),\n",
            " (23,\n",
            "  '0.265*\"good\" + 0.076*\"anything\" + 0.058*\"stop\" + 0.043*\"away\" + 0.042*\"try\" '\n",
            "  '+ 0.041*\"always\" + 0.036*\"morning\" + 0.031*\"mrs\" + 0.030*\"lie\" + '\n",
            "  '0.026*\"herbert\"'),\n",
            " (9,\n",
            "  '0.103*\"like\" + 0.087*\"give\" + 0.066*\"three\" + 0.058*\"stay\" + 0.033*\"still\" '\n",
            "  '+ 0.032*\"year\" + 0.032*\"without\" + 0.031*\"alive\" + 0.027*\"everybody\" + '\n",
            "  '0.025*\"point\"'),\n",
            " (10,\n",
            "  '0.133*\"time\" + 0.088*\"make\" + 0.088*\"leave\" + 0.064*\"much\" + 0.057*\"blow\" + '\n",
            "  '0.049*\"mad\" + 0.041*\"wrong\" + 0.038*\"carry\" + 0.023*\"better\" + '\n",
            "  '0.023*\"worth\"'),\n",
            " (29,\n",
            "  '0.558*\"sir\" + 0.064*\"happen\" + 0.049*\"clipton\" + 0.036*\"day\" + '\n",
            "  '0.025*\"murder\" + 0.023*\"prisoner\" + 0.022*\"gyeonwoo\" + 0.021*\"rank\" + '\n",
            "  '0.016*\"doubt\" + 0.015*\"tantamount\"'),\n",
            " (27,\n",
            "  '0.380*\"yes\" + 0.185*\"know\" + 0.087*\"hello\" + 0.078*\"look\" + '\n",
            "  '0.032*\"question\" + 0.025*\"ask\" + 0.018*\"dinner\" + 0.018*\"gentleman\" + '\n",
            "  '0.016*\"attention\" + 0.016*\"four\"')]\n",
            "['go', 'wait']\n",
            "Score: 0.05315301939845085\t Topic: 0.168*\"show\" + 0.083*\"maybe\" + 0.073*\"order\" + 0.059*\"many\" + 0.034*\"light\"\n",
            "Score: 0.048043105751276016\t Topic: 0.380*\"yes\" + 0.185*\"know\" + 0.087*\"hello\" + 0.078*\"look\" + 0.032*\"question\"\n",
            "Score: 0.047046683728694916\t Topic: 0.558*\"sir\" + 0.064*\"happen\" + 0.049*\"clipton\" + 0.036*\"day\" + 0.025*\"murder\"\n",
            "Score: 0.045647941529750824\t Topic: 0.133*\"time\" + 0.088*\"make\" + 0.088*\"leave\" + 0.064*\"much\" + 0.057*\"blow\"\n",
            "Score: 0.04403027892112732\t Topic: 0.103*\"like\" + 0.087*\"give\" + 0.066*\"three\" + 0.058*\"stay\" + 0.033*\"still\"\n",
            "Score: 0.043972935527563095\t Topic: 0.265*\"good\" + 0.076*\"anything\" + 0.058*\"stop\" + 0.043*\"away\" + 0.042*\"try\"\n",
            "Score: 0.037646107375621796\t Topic: 0.303*\"right\" + 0.255*\"well\" + 0.081*\"yeah\" + 0.073*\"mean\" + 0.059*\"put\"\n",
            "Score: 0.037281427532434464\t Topic: 0.254*\"say\" + 0.231*\"go\" + 0.069*\"something\" + 0.059*\"madness\" + 0.056*\"finish\"\n",
            "Score: 0.03710832819342613\t Topic: 0.128*\"ill\" + 0.118*\"call\" + 0.078*\"must\" + 0.065*\"sit\" + 0.059*\"love\"\n",
            "Score: 0.03578099608421326\t Topic: 0.196*\"one\" + 0.066*\"find\" + 0.062*\"night\" + 0.062*\"hold\" + 0.046*\"responsible\"\n",
            "Score: 0.03551342710852623\t Topic: 0.386*\"get\" + 0.309*\"thank\" + 0.053*\"ready\" + 0.048*\"knife\" + 0.048*\"matter\"\n",
            "Score: 0.0354284904897213\t Topic: 0.322*\"come\" + 0.190*\"think\" + 0.074*\"two\" + 0.035*\"police\" + 0.027*\"army\"\n",
            "Score: 0.034826356917619705\t Topic: 0.123*\"take\" + 0.079*\"wait\" + 0.077*\"major\" + 0.060*\"care\" + 0.060*\"commander\"\n",
            "Score: 0.033037930727005005\t Topic: 0.145*\"bridge\" + 0.113*\"sorry\" + 0.078*\"nothing\" + 0.064*\"mind\" + 0.062*\"need\"\n",
            "Score: 0.032884206622838974\t Topic: 0.067*\"escape\" + 0.060*\"little\" + 0.053*\"boris\" + 0.045*\"send\" + 0.040*\"medical\"\n",
            "Score: 0.03222374990582466\t Topic: 0.226*\"kill\" + 0.094*\"joyce\" + 0.056*\"easy\" + 0.042*\"new\" + 0.041*\"code\"\n",
            "Score: 0.032137755304574966\t Topic: 0.259*\"see\" + 0.154*\"thing\" + 0.070*\"old\" + 0.065*\"turn\" + 0.042*\"drink\"\n",
            "Score: 0.03075193241238594\t Topic: 0.202*\"tell\" + 0.086*\"move\" + 0.083*\"tonight\" + 0.083*\"die\" + 0.046*\"happy\"\n",
            "Score: 0.029349995777010918\t Topic: 0.204*\"want\" + 0.179*\"way\" + 0.066*\"speak\" + 0.049*\"ben\" + 0.048*\"slave\"\n",
            "Score: 0.028872203081846237\t Topic: 0.159*\"officer\" + 0.150*\"would\" + 0.083*\"lieutenant\" + 0.062*\"keep\" + 0.045*\"london\"\n",
            "Score: 0.028538383543491364\t Topic: 0.269*\"man\" + 0.086*\"okay\" + 0.083*\"sick\" + 0.045*\"blast\" + 0.038*\"shear\"\n",
            "Score: 0.02773457020521164\t Topic: 0.130*\"please\" + 0.113*\"sure\" + 0.093*\"hand\" + 0.085*\"really\" + 0.060*\"british\"\n",
            "Score: 0.02651168406009674\t Topic: 0.148*\"work\" + 0.085*\"use\" + 0.077*\"course\" + 0.068*\"evening\" + 0.050*\"saito\"\n",
            "Score: 0.02614324353635311\t Topic: 0.158*\"could\" + 0.112*\"may\" + 0.105*\"do\" + 0.055*\"idea\" + 0.045*\"baby\"\n",
            "Score: 0.025961602106690407\t Topic: 0.358*\"let\" + 0.073*\"bring\" + 0.070*\"stand\" + 0.051*\"democratic\" + 0.039*\"study\"\n",
            "Score: 0.02387387305498123\t Topic: 0.067*\"soldier\" + 0.057*\"gold\" + 0.049*\"kind\" + 0.047*\"hit\" + 0.046*\"beautiful\"\n",
            "Score: 0.023581884801387787\t Topic: 0.167*\"back\" + 0.120*\"talk\" + 0.084*\"feel\" + 0.070*\"forget\" + 0.064*\"suppose\"\n",
            "Score: 0.022412549704313278\t Topic: 0.130*\"colonel\" + 0.075*\"mama\" + 0.073*\"hope\" + 0.071*\"lovely\" + 0.069*\"boy\"\n",
            "Score: 0.02049047313630581\t Topic: 0.131*\"name\" + 0.105*\"warden\" + 0.076*\"wonder\" + 0.063*\"list\" + 0.042*\"report\"\n",
            "Score: 0.020014846697449684\t Topic: 0.229*\"help\" + 0.150*\"understand\" + 0.101*\"hear\" + 0.099*\"woman\" + 0.080*\"darle\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bLoBS2gH5v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c5fd77be-863d-4d48-cb74-14778d58c6b0"
      },
      "source": [
        "'''#Implementing Lda topic modeling\n",
        "#Splitting the inputs list itens word wise\n",
        "topic_train = []\n",
        "for i in range(len(inputs)):\n",
        "  topic_train.append(inputs[i].split())\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "#Filter out stop words in English \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "for i in range(len(topic_train)):\n",
        "  sent=\" \".join(topic_train[i])\n",
        "  sent=nlp(sent)\n",
        "  topic_train[i] = [ w.lemma_ for w in sent if w.text.lower() not in stops and len(w)>2]\n",
        "'''\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeOQ-HUhH9fx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8a9316b0-67cc-42a4-d5ec-9c766a4468dc"
      },
      "source": [
        "#Forming dictionary out of the words\n",
        "from gensim import corpora\n",
        "topic_dict = corpora.Dictionary(topic_train)\n",
        "topic_dict.filter_extremes(no_below=10, no_above=0.5)\n",
        "\n",
        "#Obtaining bow_corpus out of the topics\n",
        "bow_corpus = [topic_dict.doc2bow(doc) for doc in topic_train]\n",
        "\n",
        "#Getting tfidf corpus from tfidf model\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "#training the lda model\n",
        "from gensim import models\n",
        "NUM_TOPICS = 30\n",
        "NUM_PASSES = 60 \n",
        "ALPHA = 'auto'\n",
        "ETA = 'auto'\n",
        "# Train LDA model\n",
        "lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus, \n",
        "                                         num_topics=NUM_TOPICS, \n",
        "                                         id2word=topic_dict,\n",
        "                                         passes=NUM_PASSES, \n",
        "                                         alpha=ALPHA, \n",
        "                                         eta=ETA,\n",
        "                                         random_state=200)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RXD-yk5IBwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb00b198-3d89-4198-df18-ba1c4e033522"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "\n",
        "print(topic_train[0])\n",
        "\n",
        "#get the scores of the topics generated\n",
        "for index, score in sorted(lda_model[bow_corpus[4]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1,\n",
            "  '0.229*\"help\" + 0.150*\"understand\" + 0.101*\"hear\" + 0.099*\"woman\" + '\n",
            "  '0.080*\"darle\" + 0.042*\"people\" + 0.035*\"head\" + 0.030*\"realize\" + '\n",
            "  '0.020*\"piece\" + 0.017*\"maam\"'),\n",
            " (25,\n",
            "  '0.131*\"name\" + 0.105*\"warden\" + 0.076*\"wonder\" + 0.063*\"list\" + '\n",
            "  '0.042*\"report\" + 0.039*\"shut\" + 0.035*\"group\" + 0.034*\"principle\" + '\n",
            "  '0.033*\"fault\" + 0.026*\"wilderness\"'),\n",
            " (3,\n",
            "  '0.130*\"colonel\" + 0.075*\"mama\" + 0.073*\"hope\" + 0.071*\"lovely\" + '\n",
            "  '0.069*\"boy\" + 0.069*\"believe\" + 0.053*\"jolly\" + 0.045*\"stella\" + '\n",
            "  '0.044*\"shall\" + 0.027*\"tired\"'),\n",
            " (8,\n",
            "  '0.167*\"back\" + 0.120*\"talk\" + 0.084*\"feel\" + 0.070*\"forget\" + '\n",
            "  '0.064*\"suppose\" + 0.060*\"catch\" + 0.032*\"meg\" + 0.030*\"sort\" + '\n",
            "  '0.024*\"fever\" + 0.023*\"dysentery\"'),\n",
            " (11,\n",
            "  '0.067*\"soldier\" + 0.057*\"gold\" + 0.049*\"kind\" + 0.047*\"hit\" + '\n",
            "  '0.046*\"beautiful\" + 0.045*\"quite\" + 0.043*\"together\" + 0.043*\"first\" + '\n",
            "  '0.040*\"car\" + 0.039*\"horse\"'),\n",
            " (5,\n",
            "  '0.168*\"show\" + 0.083*\"maybe\" + 0.073*\"order\" + 0.059*\"many\" + 0.034*\"light\" '\n",
            "  '+ 0.032*\"nice\" + 0.031*\"meet\" + 0.028*\"jenkin\" + 0.023*\"able\" + '\n",
            "  '0.018*\"couple\"'),\n",
            " (20,\n",
            "  '0.358*\"let\" + 0.073*\"bring\" + 0.070*\"stand\" + 0.051*\"democratic\" + '\n",
            "  '0.039*\"study\" + 0.024*\"pick\" + 0.023*\"polytechnic\" + 0.021*\"interest\" + '\n",
            "  '0.019*\"here\" + 0.018*\"baker\"'),\n",
            " (6,\n",
            "  '0.158*\"could\" + 0.112*\"may\" + 0.105*\"do\" + 0.055*\"idea\" + 0.045*\"baby\" + '\n",
            "  '0.044*\"figure\" + 0.042*\"reasonable\" + 0.028*\"crack\" + 0.027*\"occur\" + '\n",
            "  '0.026*\"sleep\"'),\n",
            " (18,\n",
            "  '0.148*\"work\" + 0.085*\"use\" + 0.077*\"course\" + 0.068*\"evening\" + '\n",
            "  '0.050*\"saito\" + 0.048*\"train\" + 0.031*\"battalion\" + 0.030*\"serious\" + '\n",
            "  '0.029*\"english\" + 0.026*\"pill\"'),\n",
            " (0,\n",
            "  '0.130*\"please\" + 0.113*\"sure\" + 0.093*\"hand\" + 0.085*\"really\" + '\n",
            "  '0.060*\"british\" + 0.060*\"girl\" + 0.034*\"veronica\" + 0.032*\"post\" + '\n",
            "  '0.030*\"commando\" + 0.029*\"husband\"'),\n",
            " (13,\n",
            "  '0.386*\"get\" + 0.309*\"thank\" + 0.053*\"ready\" + 0.048*\"knife\" + '\n",
            "  '0.048*\"matter\" + 0.035*\"doctor\" + 0.033*\"reeve\" + 0.010*\"wake\" + '\n",
            "  '0.008*\"position\" + 0.007*\"joe\"'),\n",
            " (14,\n",
            "  '0.196*\"one\" + 0.066*\"find\" + 0.062*\"night\" + 0.062*\"hold\" + '\n",
            "  '0.046*\"responsible\" + 0.039*\"place\" + 0.036*\"ahead\" + 0.035*\"hut\" + '\n",
            "  '0.024*\"another\" + 0.022*\"dig\"'),\n",
            " (26,\n",
            "  '0.128*\"ill\" + 0.118*\"call\" + 0.078*\"must\" + 0.065*\"sit\" + 0.059*\"love\" + '\n",
            "  '0.055*\"sergeant\" + 0.052*\"never\" + 0.048*\"along\" + 0.042*\"fine\" + '\n",
            "  '0.040*\"live\"'),\n",
            " (15,\n",
            "  '0.254*\"say\" + 0.231*\"go\" + 0.069*\"something\" + 0.059*\"madness\" + '\n",
            "  '0.056*\"finish\" + 0.048*\"radio\" + 0.046*\"hey\" + 0.042*\"hurry\" + '\n",
            "  '0.034*\"moment\" + 0.030*\"listen\"'),\n",
            " (21,\n",
            "  '0.303*\"right\" + 0.255*\"well\" + 0.081*\"yeah\" + 0.073*\"mean\" + 0.059*\"put\" + '\n",
            "  '0.034*\"hell\" + 0.019*\"rule\" + 0.017*\"follow\" + 0.014*\"line\" + '\n",
            "  '0.013*\"honour\"'),\n",
            " (23,\n",
            "  '0.265*\"good\" + 0.076*\"anything\" + 0.058*\"stop\" + 0.043*\"away\" + 0.042*\"try\" '\n",
            "  '+ 0.041*\"always\" + 0.036*\"morning\" + 0.031*\"mrs\" + 0.030*\"lie\" + '\n",
            "  '0.026*\"herbert\"'),\n",
            " (9,\n",
            "  '0.103*\"like\" + 0.087*\"give\" + 0.066*\"three\" + 0.058*\"stay\" + 0.033*\"still\" '\n",
            "  '+ 0.032*\"year\" + 0.032*\"without\" + 0.031*\"alive\" + 0.027*\"everybody\" + '\n",
            "  '0.025*\"point\"'),\n",
            " (10,\n",
            "  '0.133*\"time\" + 0.088*\"make\" + 0.088*\"leave\" + 0.064*\"much\" + 0.057*\"blow\" + '\n",
            "  '0.049*\"mad\" + 0.041*\"wrong\" + 0.038*\"carry\" + 0.023*\"better\" + '\n",
            "  '0.023*\"worth\"'),\n",
            " (29,\n",
            "  '0.558*\"sir\" + 0.064*\"happen\" + 0.049*\"clipton\" + 0.036*\"day\" + '\n",
            "  '0.025*\"murder\" + 0.023*\"prisoner\" + 0.022*\"gyeonwoo\" + 0.021*\"rank\" + '\n",
            "  '0.016*\"doubt\" + 0.015*\"tantamount\"'),\n",
            " (27,\n",
            "  '0.380*\"yes\" + 0.185*\"know\" + 0.087*\"hello\" + 0.078*\"look\" + '\n",
            "  '0.032*\"question\" + 0.025*\"ask\" + 0.018*\"dinner\" + 0.018*\"gentleman\" + '\n",
            "  '0.016*\"attention\" + 0.016*\"four\"')]\n",
            "['go', 'wait']\n",
            "Score: 0.05315301939845085\t Topic: 0.168*\"show\" + 0.083*\"maybe\" + 0.073*\"order\" + 0.059*\"many\" + 0.034*\"light\"\n",
            "Score: 0.048043105751276016\t Topic: 0.380*\"yes\" + 0.185*\"know\" + 0.087*\"hello\" + 0.078*\"look\" + 0.032*\"question\"\n",
            "Score: 0.047046683728694916\t Topic: 0.558*\"sir\" + 0.064*\"happen\" + 0.049*\"clipton\" + 0.036*\"day\" + 0.025*\"murder\"\n",
            "Score: 0.045647941529750824\t Topic: 0.133*\"time\" + 0.088*\"make\" + 0.088*\"leave\" + 0.064*\"much\" + 0.057*\"blow\"\n",
            "Score: 0.04403027892112732\t Topic: 0.103*\"like\" + 0.087*\"give\" + 0.066*\"three\" + 0.058*\"stay\" + 0.033*\"still\"\n",
            "Score: 0.043972935527563095\t Topic: 0.265*\"good\" + 0.076*\"anything\" + 0.058*\"stop\" + 0.043*\"away\" + 0.042*\"try\"\n",
            "Score: 0.037646107375621796\t Topic: 0.303*\"right\" + 0.255*\"well\" + 0.081*\"yeah\" + 0.073*\"mean\" + 0.059*\"put\"\n",
            "Score: 0.037281427532434464\t Topic: 0.254*\"say\" + 0.231*\"go\" + 0.069*\"something\" + 0.059*\"madness\" + 0.056*\"finish\"\n",
            "Score: 0.03710832819342613\t Topic: 0.128*\"ill\" + 0.118*\"call\" + 0.078*\"must\" + 0.065*\"sit\" + 0.059*\"love\"\n",
            "Score: 0.03578099608421326\t Topic: 0.196*\"one\" + 0.066*\"find\" + 0.062*\"night\" + 0.062*\"hold\" + 0.046*\"responsible\"\n",
            "Score: 0.03551342710852623\t Topic: 0.386*\"get\" + 0.309*\"thank\" + 0.053*\"ready\" + 0.048*\"knife\" + 0.048*\"matter\"\n",
            "Score: 0.0354284904897213\t Topic: 0.322*\"come\" + 0.190*\"think\" + 0.074*\"two\" + 0.035*\"police\" + 0.027*\"army\"\n",
            "Score: 0.034826356917619705\t Topic: 0.123*\"take\" + 0.079*\"wait\" + 0.077*\"major\" + 0.060*\"care\" + 0.060*\"commander\"\n",
            "Score: 0.033037930727005005\t Topic: 0.145*\"bridge\" + 0.113*\"sorry\" + 0.078*\"nothing\" + 0.064*\"mind\" + 0.062*\"need\"\n",
            "Score: 0.032884206622838974\t Topic: 0.067*\"escape\" + 0.060*\"little\" + 0.053*\"boris\" + 0.045*\"send\" + 0.040*\"medical\"\n",
            "Score: 0.03222374990582466\t Topic: 0.226*\"kill\" + 0.094*\"joyce\" + 0.056*\"easy\" + 0.042*\"new\" + 0.041*\"code\"\n",
            "Score: 0.032137755304574966\t Topic: 0.259*\"see\" + 0.154*\"thing\" + 0.070*\"old\" + 0.065*\"turn\" + 0.042*\"drink\"\n",
            "Score: 0.03075193241238594\t Topic: 0.202*\"tell\" + 0.086*\"move\" + 0.083*\"tonight\" + 0.083*\"die\" + 0.046*\"happy\"\n",
            "Score: 0.029349995777010918\t Topic: 0.204*\"want\" + 0.179*\"way\" + 0.066*\"speak\" + 0.049*\"ben\" + 0.048*\"slave\"\n",
            "Score: 0.028872203081846237\t Topic: 0.159*\"officer\" + 0.150*\"would\" + 0.083*\"lieutenant\" + 0.062*\"keep\" + 0.045*\"london\"\n",
            "Score: 0.028538383543491364\t Topic: 0.269*\"man\" + 0.086*\"okay\" + 0.083*\"sick\" + 0.045*\"blast\" + 0.038*\"shear\"\n",
            "Score: 0.02773457020521164\t Topic: 0.130*\"please\" + 0.113*\"sure\" + 0.093*\"hand\" + 0.085*\"really\" + 0.060*\"british\"\n",
            "Score: 0.02651168406009674\t Topic: 0.148*\"work\" + 0.085*\"use\" + 0.077*\"course\" + 0.068*\"evening\" + 0.050*\"saito\"\n",
            "Score: 0.02614324353635311\t Topic: 0.158*\"could\" + 0.112*\"may\" + 0.105*\"do\" + 0.055*\"idea\" + 0.045*\"baby\"\n",
            "Score: 0.025961602106690407\t Topic: 0.358*\"let\" + 0.073*\"bring\" + 0.070*\"stand\" + 0.051*\"democratic\" + 0.039*\"study\"\n",
            "Score: 0.02387387305498123\t Topic: 0.067*\"soldier\" + 0.057*\"gold\" + 0.049*\"kind\" + 0.047*\"hit\" + 0.046*\"beautiful\"\n",
            "Score: 0.023581884801387787\t Topic: 0.167*\"back\" + 0.120*\"talk\" + 0.084*\"feel\" + 0.070*\"forget\" + 0.064*\"suppose\"\n",
            "Score: 0.022412549704313278\t Topic: 0.130*\"colonel\" + 0.075*\"mama\" + 0.073*\"hope\" + 0.071*\"lovely\" + 0.069*\"boy\"\n",
            "Score: 0.02049047313630581\t Topic: 0.131*\"name\" + 0.105*\"warden\" + 0.076*\"wonder\" + 0.063*\"list\" + 0.042*\"report\"\n",
            "Score: 0.020014846697449684\t Topic: 0.229*\"help\" + 0.150*\"understand\" + 0.101*\"hear\" + 0.099*\"woman\" + 0.080*\"darle\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8hIffR10Vmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "718494ad-47d9-4544-ac02-3f1606e119d0"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    x = WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "    return stemmer.stem(x)\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyiH5Pm1UbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31a7b87e-e2f5-411d-c7ae-9de7fd9101f7"
      },
      "source": [
        "import random\n",
        "def extract_topic(input):\n",
        "\n",
        "  c = 0\n",
        "  bow_vector = topic_dict.doc2bow(preprocess(input))\n",
        "  for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "      wp = lda_model.show_topic(index)\n",
        "      topic_keywords = \", \".join([word for word, prop in wp])\n",
        "      '''if c == random.randint(0,30):\n",
        "         break\n",
        "      c += 1'''\n",
        "  return topic_keywords\n",
        "\n",
        "#input = 'We are really blessed by God with a caring and loving mother. Without mothers our lives are nothing. We are so lucky as we have a mother. We give lots of special gifts to our mother and she gives us lots of love and care.'\n",
        "extract_topic('i will kill him')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hand, minut, jump, fast, murder, peopl, kiss, save, air, realiz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IxP0c3t144c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "060f456d-b8c7-4cfc-fc57-a3f927f3114e"
      },
      "source": [
        "good=extract_topic('i want to kill him').split(', ')\n",
        "print(input_seq_pad[0], response_seq_pad[0])\n",
        "print(extract_topic(inputs[0]),\"----\",extract_topic(responses[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 28  79   7 106   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0] [   0   21   40   20 6979    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "hand, minut, jump, fast, murder, peopl, kiss, save, air, realiz ---- hand, minut, jump, fast, murder, peopl, kiss, save, air, realiz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKh0GIw41-Ib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "f0029259-c424-4d01-8bfe-7f706ee7279d"
      },
      "source": [
        "###\n",
        "new_input_seq_pad = []\n",
        "new_response_seq_pad = []\n",
        "\n",
        "'''for i in range(len(inputs)):\n",
        "    topics = extract_topic(inputs[i])\n",
        "    topics = topics.split(', ')\n",
        "    topic_seq = tokenizer.texts_to_sequences([topics[:num_topic_words]])\n",
        "    topic_seq_pad = pad_sequences(topic_seq, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "    temp = np.concatenate((input_seq_pad[i], np.array(topic_seq_pad[0])))\n",
        "    new_input_seq_pad.append(temp) '''\n",
        "for i in range(len(inputs)):\n",
        "    topics = extract_topic(inputs[i])\n",
        "    topics_responses=extract_topic(responses[i])\n",
        "    topics = topics.split(', ')\n",
        "    topics_responses=topics_responses.split(', ')\n",
        "    topic_seq = tokenizer.texts_to_sequences([topics[:num_topic_words]])\n",
        "    topic_seq_responses = tokenizer.texts_to_sequences([topics_responses[:num_topic_words]])\n",
        "    topic_seq_pad = pad_sequences(topic_seq, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "    topic_seq_pad_responses = pad_sequences(topic_seq_responses, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "    temp = np.concatenate((input_seq_pad[i], np.array(topic_seq_pad[0])))\n",
        "    temp_final=np.concatenate((response_seq_pad[i], np.array(topic_seq_pad_responses[0])))\n",
        "    new_input_seq_pad.append(temp)\n",
        "    new_response_seq_pad.append(temp_final) \n",
        "\n",
        "    \n",
        "print(len(word_index))\n",
        "print(inputs[0], responses[0])\n",
        "print(new_input_seq_pad[0], new_response_seq_pad[0])\n",
        "print(len(new_input_seq_pad))\n",
        "print(new_input_seq_pad[1], new_response_seq_pad[1])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6978\n",
            "im going to wait startsent here we go endsent\n",
            "[ 28  79   7 106   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0 316   1 495 457   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0] [   0   21   40   20 6979    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  316    1\n",
            "  495  457    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "101770\n",
            "[  40  784    7  470   21  138  192  281   60  119 1063   26  280    0\n",
            "    0    0    0    0    0    0  316    1  495  457    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0] [   0   21   40   20 6979    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  316    1\n",
            "  495  457    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joksr6RYRiJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "73c46773-d769-4995-9f5d-509ea18cef1c"
      },
      "source": [
        "print(topic_seq_pad_responses)\n",
        "print(new_input_seq_pad[1], new_response_seq_pad[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[316   1 495 457   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]]\n",
            "[  40  784    7  470   21  138  192  281   60  119 1063   26  280    0\n",
            "    0    0    0    0    0    0  316    1  495  457    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0] [   0   21   40   20 6979    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  316    1\n",
            "  495  457    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rMpDyOC5RF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0e66b88f-1de3-410a-eac9-7a874a3420e8"
      },
      "source": [
        "def get_topic(input):\n",
        "\n",
        "    input_topic = extract_topic(input)\n",
        "    input_seq = tokenizer.texts_to_sequences([input_topic])\n",
        "    input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')\n",
        "    return input_seq_pad[0]\n",
        "\n",
        "get_topic('I am happy')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([316,   1, 495, 457, 652,   1, 437, 540, 423,   1,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lns1a64j5YkR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5ee4202c-70c8-4bce-f789-4e89151c2f0e"
      },
      "source": [
        "\n",
        "\"\"\"print(input_seq_pad.shape)\n",
        "X_train = input_seq_pad[:500]\n",
        "y_train = response_seq_pad[:500]\"\"\"\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for j in range(50000):\n",
        "    X_train.append(new_input_seq_pad[j])\n",
        "    y_train.append(new_response_seq_pad[j])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(embedding_dim)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "dataset = (inputs, responses)\n",
        "print(len(inputs))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "(50000, 40)\n",
            "(50000, 60)\n",
            "101770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khpeNdnLJF-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "18e8f937-a454-4105-c4cc-bf022a51e66c"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense \n",
        "\n",
        "max_sequence_len = 40\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(1024)\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 40), (64, 60)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKhnyAYXKRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, hidden_size=1024, max_sequence_len=40, batch_size=batch_size, embedding_dim=300, vocab_size=vocab_size+1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embedding_layer = Embedding(\n",
        "            input_dim=self.vocab_size, output_dim=self.embedding_dim, weights=[embeddings_matrix], trainable=False)\n",
        "        self.GRU_1 = GRU(units=hidden_size, return_sequences=True,recurrent_initializer='glorot_uniform')\n",
        "        self.GRU_2 = GRU(units=hidden_size,\n",
        "                         return_sequences=True, return_state=True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def initial_hidden_state(self):\n",
        "        return tf.zeros(shape=(self.batch_size, self.hidden_size))\n",
        "\n",
        "    def call(self, x, initial_state, training=False):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = self.GRU_1(x, initial_state=initial_state)\n",
        "        x, hidden_state = self.GRU_2(x)\n",
        "        return x, hidden_state"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdYxWTlTXOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, hidden_size=256):\n",
        "        super(Attention, self).__init__()\n",
        "        self.fc1 = Dense(units=hidden_size)\n",
        "        self.fc2 = Dense(units=hidden_size)\n",
        "        self.fc3 = Dense(units=1)\n",
        "\n",
        "    def call(self, encoder_output, hidden_state, training=False):\n",
        "        '''hidden_state : h(t-1)'''\n",
        "        y_hidden_state = tf.expand_dims(hidden_state, axis=1)\n",
        "        y_hidden_state = self.fc1(y_hidden_state)\n",
        "        y_enc_out = self.fc2(encoder_output)\n",
        "\n",
        "        y = tf.keras.backend.tanh(y_enc_out + y_hidden_state)\n",
        "        attention_score = self.fc3(y)\n",
        "        attention_weights = tf.keras.backend.softmax(attention_score, axis=1)\n",
        "\n",
        "        context_vector = tf.multiply(encoder_output, attention_weights)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiPIuT73XR18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, hidden_size=1024, max_sequence_len=40, batch_size=batch_size, embedding_dim=300, vocab_size=vocab_size+1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "        self.embedding_layer = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, weights=[embeddings_matrix], trainable=False)\n",
        "        self.GRU = GRU(units=hidden_size,\n",
        "                       return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "        self.attention = Attention(hidden_size=self.hidden_size)\n",
        "        self.fc = Dense(units=self.vocab_size)\n",
        "\n",
        "    def initial_hidden_state(self):\n",
        "        return tf.zeros(shape=(self.batch_size, self.hidden_size))\n",
        "\n",
        "    def call(self, x, encoder_output, hidden_state, training=False):\n",
        "        x = self.embedding_layer(x)\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            encoder_output, hidden_state, training=training)\n",
        "        contect_vector = tf.expand_dims(context_vector, axis=1)\n",
        "        x = tf.concat([x, contect_vector], axis=-1)\n",
        "        x, curr_hidden_state = self.GRU(x)\n",
        "        x = tf.reshape(x, shape=[self.batch_size, -1])\n",
        "        x = self.fc(x)\n",
        "        return x, curr_hidden_state, attention_weights"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhnb0Qs4XUTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
        "train_accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    mask = 1 - tf.cast(tf.equal(y_true, 0), 'float32')\n",
        "    return tf.reduce_mean(loss * mask)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HKOw2aqXZTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_step(inputs, responses):    \n",
        "    with tf.GradientTape() as Tape:\n",
        "        encoder_init_state = encoder.initial_hidden_state()\n",
        "        encoder_output, encoder_hidden_state = encoder(inputs, encoder_init_state, training=True)\n",
        "        decoder_hidden = encoder_hidden_state\n",
        "        loss = 0\n",
        "        acc = []\n",
        "        current_word = tf.expand_dims(responses[:, 0], axis=1)\n",
        "        for word_idx in range(1, max_sequence_len):\n",
        "            next_word = responses[:, word_idx]\n",
        "            logits, decoder_hidden, attention_weights = decoder(current_word, encoder_output, decoder_hidden)\n",
        "            loss += loss_function(next_word, logits)\n",
        "            acc.append(train_accuracy(next_word, logits))\n",
        "            current_word = tf.expand_dims(next_word, axis=1)\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = Tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss, tf.reduce_mean(acc)\n",
        "\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3UFOXOzXdar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "outputId": "7a53da2e-cfc3-4f85-afe4-2bbac6137539"
      },
      "source": [
        "epochs = 15\n",
        "NUM_SAMPLES = len(X_train)\n",
        "num_steps = NUM_SAMPLES // batch_size\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f'Epoch {epoch}/{epochs}')\n",
        "    ep_loss = []\n",
        "    ep_acc = []\n",
        "    progbar = tf.keras.utils.Progbar(target=num_steps, stateful_metrics=[\n",
        "                                     'curr_loss', 'curr_accuracy'], unit_name='batch')\n",
        "\n",
        "    for step, example in enumerate(train_dataset):\n",
        "        inputs = example[0]\n",
        "        responses = example[1]\n",
        "        loss, acc = training_step(inputs, responses)\n",
        "        loss /= responses.shape[1]\n",
        "        ep_loss.append(loss)\n",
        "        ep_acc.append(acc)\n",
        "        progbar.update(\n",
        "            step + 1, values=[('curr_loss', loss), ('curr_accuracy', acc)])\n",
        "\n",
        "    print(f'Metrics after epoch {epoch} : Loss => {np.mean(ep_loss):.3f} | Accuracy => {np.mean(ep_acc):.3f}')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "781/781 [==============================] - 537s 687ms/batch - curr_loss: 0.1453 - curr_accuracy: 0.9224\n",
            "Metrics after epoch 1 : Loss => 0.185 | Accuracy => 0.852\n",
            "Epoch 2/15\n",
            "781/781 [==============================] - 537s 688ms/batch - curr_loss: 0.1213 - curr_accuracy: 0.9377\n",
            "Metrics after epoch 2 : Loss => 0.134 | Accuracy => 0.932\n",
            "Epoch 3/15\n",
            "781/781 [==============================] - 536s 686ms/batch - curr_loss: 0.1087 - curr_accuracy: 0.9443\n",
            "Metrics after epoch 3 : Loss => 0.119 | Accuracy => 0.941\n",
            "Epoch 4/15\n",
            "781/781 [==============================] - 534s 684ms/batch - curr_loss: 0.1051 - curr_accuracy: 0.9485\n",
            "Metrics after epoch 4 : Loss => 0.109 | Accuracy => 0.946\n",
            "Epoch 5/15\n",
            "781/781 [==============================] - 535s 685ms/batch - curr_loss: 0.1005 - curr_accuracy: 0.9516\n",
            "Metrics after epoch 5 : Loss => 0.101 | Accuracy => 0.950\n",
            "Epoch 6/15\n",
            "781/781 [==============================] - 536s 686ms/batch - curr_loss: 0.0950 - curr_accuracy: 0.9542\n",
            "Metrics after epoch 6 : Loss => 0.091 | Accuracy => 0.953\n",
            "Epoch 7/15\n",
            "781/781 [==============================] - 535s 685ms/batch - curr_loss: 0.0723 - curr_accuracy: 0.9564\n",
            "Metrics after epoch 7 : Loss => 0.080 | Accuracy => 0.955\n",
            "Epoch 8/15\n",
            "781/781 [==============================] - 536s 686ms/batch - curr_loss: 0.0709 - curr_accuracy: 0.9584\n",
            "Metrics after epoch 8 : Loss => 0.069 | Accuracy => 0.957\n",
            "Epoch 9/15\n",
            "781/781 [==============================] - 537s 687ms/batch - curr_loss: 0.0570 - curr_accuracy: 0.9602\n",
            "Metrics after epoch 9 : Loss => 0.060 | Accuracy => 0.959\n",
            "Epoch 10/15\n",
            "781/781 [==============================] - 535s 685ms/batch - curr_loss: 0.0493 - curr_accuracy: 0.9617\n",
            "Metrics after epoch 10 : Loss => 0.054 | Accuracy => 0.961\n",
            "Epoch 11/15\n",
            "781/781 [==============================] - 534s 684ms/batch - curr_loss: 0.0476 - curr_accuracy: 0.9631\n",
            "Metrics after epoch 11 : Loss => 0.050 | Accuracy => 0.962\n",
            "Epoch 12/15\n",
            "781/781 [==============================] - 534s 683ms/batch - curr_loss: 0.0474 - curr_accuracy: 0.9643\n",
            "Metrics after epoch 12 : Loss => 0.046 | Accuracy => 0.964\n",
            "Epoch 13/15\n",
            "781/781 [==============================] - 534s 684ms/batch - curr_loss: 0.0413 - curr_accuracy: 0.9654\n",
            "Metrics after epoch 13 : Loss => 0.043 | Accuracy => 0.965\n",
            "Epoch 14/15\n",
            "781/781 [==============================] - 534s 684ms/batch - curr_loss: 0.0489 - curr_accuracy: 0.9664\n",
            "Metrics after epoch 14 : Loss => 0.041 | Accuracy => 0.966\n",
            "Epoch 15/15\n",
            "781/781 [==============================] - 534s 684ms/batch - curr_loss: 0.0437 - curr_accuracy: 0.9673\n",
            "Metrics after epoch 15 : Loss => 0.039 | Accuracy => 0.967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6odunK1Xfo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def translate_sentence(sentence):\n",
        "    topic_sent = get_topic(sentence)\n",
        "    sentence = preprocess_sent([sentence])\n",
        "    new_sentence = np.concatenate((np.array(sentence[0]), np.array(topic_sent)))\n",
        "    enc_init = tf.zeros(shape=[1, 1024])\n",
        "    enc_out, enc_hidden = encoder(sentence, enc_init)\n",
        "    decoder.batch_size = 1\n",
        "    tokenizer.index_word[0] = ''\n",
        "    decoded = []\n",
        "    att = []\n",
        "    current_word = tf.expand_dims([word_index['startsent']], axis=0) \n",
        "    decoder_hidden = enc_hidden\n",
        "    for word_idx in range(1, max_sequence_len):\n",
        "        logits, decoder_hidden, attention_weights = decoder(current_word, enc_out, decoder_hidden)\n",
        "        decoded_idx = np.argmax(logits)\n",
        "        if index_word[decoded_idx] == 'endsent':\n",
        "            break\n",
        "        decoded.append(tokenizer.index_word[decoded_idx])\n",
        "        att.append(attention_weights.numpy().squeeze())\n",
        "        current_word = tf.expand_dims([decoded_idx], axis=0)\n",
        "    return ' '.join(decoded), att"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re2-tN5bXiFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "035854c0-50d0-4bc4-b92e-4c28f8e67030"
      },
      "source": [
        "\n",
        "sentences = ['who are you','what do you do','where are you going','who do you live with','are you single','i know a good cafe here','you are very rude','is modi god','i want to be free']\n",
        "\n",
        "for inp_sentence in sentences:\n",
        "    inp_array = inp_sentence.split()\n",
        "    inp_len = len(inp_sentence.split())\n",
        "    trans_sentence, attention_weights = translate_sentence(inp_sentence)\n",
        "    trans_array = trans_sentence.split()\n",
        "    trans_len = len(trans_array)\n",
        "    print('INPUT : ', inp_sentence)\n",
        "    print('RESPONSE : ', trans_sentence)\n",
        "    print('-'*30)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT :  who are you\n",
            "RESPONSE :  how old are you\n",
            "------------------------------\n",
            "INPUT :  what do you do\n",
            "RESPONSE :  nurse\n",
            "------------------------------\n",
            "INPUT :  where are you going\n",
            "RESPONSE :  im on duty\n",
            "------------------------------\n",
            "INPUT :  who do you live with\n",
            "RESPONSE :  how old are you\n",
            "------------------------------\n",
            "INPUT :  are you single\n",
            "RESPONSE :  what\n",
            "------------------------------\n",
            "INPUT :  i know a good cafe here\n",
            "RESPONSE :  i cant\n",
            "------------------------------\n",
            "INPUT :  you are very rude\n",
            "RESPONSE :  whats it good for\n",
            "------------------------------\n",
            "INPUT :  is modi god\n",
            "RESPONSE :  hes going to live\n",
            "------------------------------\n",
            "INPUT :  i want to be free\n",
            "RESPONSE :  call the doctor\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kAZWPjqDHxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}